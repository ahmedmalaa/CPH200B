{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27cc5509-75ad-4148-93b8-088b97ad1b17",
   "metadata": {},
   "source": [
    "<p align=center>\n",
    "<img src=\"assets/cphbanner.png\" width=1280>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c79aea8-f578-4310-9e04-46fed86f21f1",
   "metadata": {},
   "source": [
    "# **Project 2: Machine Learning-based Causal Effect Estimation [30 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f62d3c-682e-44df-bda2-c04125cdb7a8",
   "metadata": {},
   "source": [
    "Causality is a central concept in science and medicine. The fundamental goal of medical research is to understand whether an intervention has a causal effect on the outcomes of a specific patient population. Answering\n",
    "such causal questions require randomization of the intervention and comparison of outcomes of treated patients against a control group. However, conducting randomized trials is not always feasible, and we often\n",
    "need to estimate causal effects from observational data. The goal of this project is to test your understanding\n",
    "of some of the causal inference concepts studied in class, as well as apply and develop machine learning-based\n",
    "methods for causal effect estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f2cb2a-942c-4462-8615-073b7a5e8363",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "This is <u> not a group project and students will be graded individually</u>. This project has two deliverables:\n",
    "\n",
    "- A report summarizing your results. The report should include point-by-point answers to the questions below. Please submit your report (in pdf format) via the [bcourses](https://bcourses.berkeley.edu/courses/1531248) website for CPH200B.\n",
    "- A zip file with your codebase. You can fork this repo and add your code to it. Please submit both your code and report using the [Gradescope](https://www.gradescope.com/courses/684408) website for CPH200B. You will get feedback on both your report and code via Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345becba-b634-48c3-9130-b7071af5e878",
   "metadata": {},
   "source": [
    "**Please submit your report and code by <u> Thursday 2/27 11:59 PST </u>.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac5b5a-a269-46be-bf9a-a300b535809f",
   "metadata": {},
   "source": [
    "## Task 2.1: Warm-up Exercise: Hypothesis Testing & Confounding [8 pts]\n",
    "\n",
    "The most basic form of causal inference involves comparing survival curves in different groups stratified by an\n",
    "intervention of interest. In this task, we will implement hypothesis testing methods to examine whether differences between the outcomes of treated and untreated patients are statistically significant, and whether these\n",
    "difference reflect the causal effect of the intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fdb3da-c55d-46a3-a277-82260a5b8298",
   "metadata": {},
   "source": [
    "####  Setup and Dataset\n",
    "\n",
    "The dataset we will use in this task was extracted from the electronic health records (EHRs) of 299 heart failure patients from the Faisalabad Institute of Cardiology and at the Allied Hospital in Faisalabad (Punjab, Pakistan), during April–December 2015. The cohort included 105 women and 194 men, and their ages range between 40 and 95 years old. All 299 patients had left ventricular systolic dysfunction and had previous heart failures (HF) that put them in classes III or IV of New York Heart Association (NYHA) classification of the stages of heart failure. The dataset contains 13 features, which report clinical, body, and lifestyle information. The patients were followed up for 130 days on average (maximum follow-up period was 285 days). The event of interest was death during the follow-up period. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3b06b-9959-4feb-8700-e22e2cee7918",
   "metadata": {},
   "source": [
    "For all the tasks below, we will use the UNOS heart transplant [1] dataset from Project 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb19733-6638-43be-9f46-456a7b4fe03b",
   "metadata": {},
   "source": [
    "#### Tasks and Deliverables\n",
    "\n",
    "Please complete the following tasks. Your report should include the results for each task (i.e., tables or plots) along with your answers to the questions associated with each task.\n",
    "\n",
    "**Task 2.1.1 [2 pts].** Implement the Log-Rank test from scratch in Python. Using the UNOS dataset, apply your implemented test to check whether the survival outcomes of patients on ventricular assist device (VAD) support differ from those of patients without VAD support. Compare your results with the built-in function for Log-Rank testing in the lifelines library.\n",
    "\n",
    "**Task 2.1.2 [2 pts].** Propose a method to determine if there are confounders in the UNOS dataset for the effect of VAD support on survival outcomes. List all detected confounders. Additionally, consider the possibility of hidden confounders not collected in UNOS data. Specify potential hidden confounders for the VAD intervention and explain why these variables may have a confounding effect.\n",
    "\n",
    "**Task 2.1.3 [2 pts].** For the comparison of survival curves to have a causal interpretation, we need to adjust for confounding variables that may cause the patient groups being compared to have different clinical features. Propose a propensity-weighted version of the Kaplan-Meier estimator you implemented in Project 1 that adjusts for confounding variables. Plot the propensity-weighted Kaplan-Meier curves in patients with VAD and without VAD. Compare this plot with the survival curves of both groups using the standard (unadjusted) Kaplan-Meier estimators.\n",
    "\n",
    "**Task 2.1.4 [2 pts].** Propose a propensity-weighted version of the Long-Rank test. Apply this test to check whether the survival outcomes of patients on VAD support differ from those of patients without VAD. Compare the result of this test with the unadjusted test you implemented in Task 2.1.1. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21442ab-caf2-4dae-acf9-ca225a272d9d",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f1a84b-14d3-489c-b2a9-7c4195f49c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Write your code here <<"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4cbeb1-0b85-4928-bb6c-cb310c2c5e22",
   "metadata": {},
   "source": [
    "## Task 2.2 Estimating Average Treatment Effects [8 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330776f8-9ef0-4ad6-94a6-1132d0095f6a",
   "metadata": {},
   "source": [
    "#### Clinical Background, Dataset, and Setup\n",
    "\n",
    "In this task, we will use individual patient data from the International Stroke Trial (IST), one of the largest randomized trials ever conducted in acute stroke [2]. The trial investigated the impact of aspirin and subcutaneous heparin on patients with acute ischemic stroke, with treatment randomization within 48 hours of symptom onset. The trial findings indicated no effect of both aspirin and heparin on 14-day and 6-month mortality. The trial protocol and data dictionary have been provided to you. The original IST data lacks confounding as it was generated through a randomized trial. The instructor introduced confounding artificially by filtering patients out of the trial using a random function that depends on the patient features. The resulting dataset mimics an observational dataset where treatment is assigned through a mechanism that depends on patient features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb62de-06c8-49dc-b96a-c9fa029ed05f",
   "metadata": {},
   "source": [
    "#### Tasks and Deliverables\n",
    "\n",
    "Estimate the average effect of aspirin and heparin on 14-day mortality using the following estimators. For each estimator, construct a 95% confidence interval around your estimate. Compare your estimates with those of the original trial and provide commentary on the results.\n",
    "\n",
    "**Task 2.2.1 [2 pts].** A standard difference-in-means estimator.\n",
    "\n",
    "**Task 2.2.2 [2 pts].** An inverse propensity weighting (IPW) estimator.\n",
    "\n",
    "**Task 2.2.3 [2 pts].** A covariate adjustment estimator using a Gradient Boosting model with T-learner, Slearner, and X-learner architectures.\n",
    "\n",
    "**Task 2.2.4 [2 pts].** A doubly-robust estimator that combines the propensity model from Task 2.2.2 and the outcomes model based on the X-learner in Task 2.2.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c95042-1b06-4135-ab5d-7c20ff0b9b1e",
   "metadata": {},
   "source": [
    "The IST data was shared with you via UCSF Box. You can find the modified trial data with artificially-generated confounding in \"IST_observational.csv\". The data dictionary is provided in \"IST_variables.csv\" and the trial protocol can be found in IST-1_protocol.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1594bf-045d-45bb-82dd-19e94065452a",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e897d782-4efb-4b59-9a55-12cf4bdaeb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Write your code here <<"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc70b2d-cc66-42d0-8b37-8251f9e66636",
   "metadata": {},
   "source": [
    "## Task 2.3 Estimating Heterogeneous Treatment Effects using Deep Counterfactual Regression [14 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15c96b-d302-45d9-96fe-49aca5ce384c",
   "metadata": {},
   "source": [
    "In this task, we will explore the application of concepts from the machine learning literature to estimate heterogeneous treatment effects. The seminal work in [3] establishes a link between estimating treatment effects and the domain adaptation problem in machine learning. Using this insight, the authors repurpose ideas from domain adaptation literature to create a new deep learning model for estimating the conditional average treatment effects (CATE) function. The core idea of their algorithm is to eliminate the confounding effect by learning a representation Φ of the input features X that aligns the distribution of treated and control populations, Φ(X|T = 1) and Φ(X|T = 0), in the representation space, referred to by the authors as a “balancing” representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d947fe-06e0-4809-ba08-246f42b83438",
   "metadata": {},
   "source": [
    "Please read the paper carefully and complete the following tasks.\n",
    "\n",
    "**Task 2.3.1 [5 pts].** Implement the T ARNet, CF RMMD, and CF RW ASS models proposed in [3] in PyTorch. Evaluate the performance of all models using the semi-synthetic Infant Health and Development Program (IHDP) benchmark dataset introduced in [4] and described in Sectioon 5.1 in [3]. A version of the IHDP dataset was shared with you via UCSF Box.\n",
    "\n",
    "**Task 2.3.2 [2 pts].** Visualize the treated and control features in IHDP before and after applying the balancing representation Φ(.) using t-SNE. Comment on the results.\n",
    "\n",
    "**Task 2.3.3 [2 pts].** Show the impact of the scaling parameter α (Eq. (3) in [3]) on the loss function on the test set for both the Wasserstein and Maximum Mean Discrepancy (MMD) regularizers.\n",
    "\n",
    "**Task 2.3.4 [3 pts].** Use the T ARNet, CF RMMD, and CF RW ASS models to estimate average treatment effects using the IST data in Task 2.2. Assess the alignment of your estimates with the trial results and compare them to the estimators in Tasks 2.2.3 and 2.2.4.\n",
    "\n",
    "**Task 2.3.5 [2 pts].** Identify any limitations of the balancing representations approach. Propose potential solutions to address these limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481eae55-45ef-46b9-9d45-a0a63f1da3d8",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa53cdd-d1af-4c4e-99ad-cea20184c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Write your model loading function here <<"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafc12b-0755-4289-aa17-6fe5e254df45",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Weiss, Eric S., Lois U. Nwakanma, Stuart B. Russell, John V. Conte, and Ashish S. Shah. “Outcomes in\n",
    "bicaval versus biatrial techniques in heart transplantation: an analysis of the UNOS database.” The Journal\n",
    "of heart and lung transplantation, vol. 27, no. 2 (2008): 178-183.\n",
    "\n",
    "[2] International Stroke Trial Collaborative Group. “The International Stroke Trial (IST): a randomised trial\n",
    "of aspirin, subcutaneous heparin, both, or neither among 19 435 patients with acute ischaemic stroke.” The\n",
    "Lancet, 349.9065 (1997): 1569-1581.\n",
    "\n",
    "[3] Shalit, U., Johansson, F. D., and D. Sontag. “Estimating individual treatment effect: generalization\n",
    "bounds and algorithms.” In International conference on machine learning (pp. 3076-3085). 2017.\n",
    "\n",
    "[4] Hill, J. L. (2011). Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1), 217-240."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
